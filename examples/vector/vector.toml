# Vector Configuration
# Consumes logs from Kafka and sends to Elasticsearch

# ============ DATA DIRECTORY ============
data_dir = "/var/lib/vector"

# ============ SOURCES ============

# Kafka source for application logs
[sources.kafka_logs]
type = "kafka"
bootstrap_servers = "kafka:29092"
group_id = "vector-logs"
topics = ["logs.application"]
decoding.codec = "json"
auto_offset_reset = "earliest"

# ============ TRANSFORMS ============

# Add metadata and parse timestamps
[transforms.parse_logs]
type = "remap"
inputs = ["kafka_logs"]
source = '''
# Parse timestamp if exists
if exists(.timestamp) {
  .@timestamp = parse_timestamp!(.timestamp, format: "%+")
} else {
  .@timestamp = now()
}

# Add vector metadata
.pipeline = "vector"
.ingested_at = now()

# Ensure required fields
.service = .service ?? "unknown"
.level = .level ?? "info"
.environment = .environment ?? "unknown"
'''

# Add Elasticsearch index info
[transforms.route_logs]
type = "remap"
inputs = ["parse_logs"]
source = '''
# Create daily index name
.index = "logs-" + format_timestamp!(.@timestamp, format: "%Y.%m.%d")
'''

# ============ SINKS ============

# Elasticsearch sink for logs
[sinks.elasticsearch_logs]
type = "elasticsearch"
inputs = ["route_logs"]
endpoints = ["http://elasticsearch:9200"]
bulk.index = "{{ index }}"
encoding.except_fields = ["index"]

# Console sink for debugging (remove in production)
[sinks.console_debug]
type = "console"
inputs = ["route_logs"]
encoding.codec = "json"
# Uncomment below to disable console output
# healthcheck.enabled = false

# ============ API ============
[api]
enabled = true
address = "0.0.0.0:8686"
